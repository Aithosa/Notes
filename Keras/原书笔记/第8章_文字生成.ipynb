{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import jieba\n",
    "plt.rcParams['figure.figsize']=(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(82832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用《四世同堂》这部小说作为训练集。读者也可以选用其他长篇小说，或者爬取网上新闻作为训练集。通常句式和语言比较有自己风格的长篇小说训练起来相对容易产出好的结果，就像我们读了武侠小说就比较容易学那种写法一个道理。因此读者也不妨选用名家的武侠小说，比如金庸全集等来训练自己的模型。网上爬取的新闻则具有数据量大，风格一致的特点，也适合用来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#不符合下面固定句长设定的程序要求\n",
    "#但是可用于计算平均句长\n",
    "fileopen = open(\"./data/四世同堂.txt\", encoding='utf-8')\n",
    "with fileopen as fo:\n",
    "    alltext0 = fo.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alltext = open(\"./data/四世同堂.txt\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3545"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(alltext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先按照单个字来建模。首先把所有的字符抽取出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n较naive的做法\\ncharset = {}\\nid = 0\\nfor line in alltext:\\n    length = len(line)\\n    for k in range(length):\\n        w = line[k]\\n        if not w in charset:            \\n            charset[w]=id\\n            id+=1\\n            \\nprint(len(charset))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "较naive的做法\n",
    "charset = {}\n",
    "id = 0\n",
    "for line in alltext:\n",
    "    length = len(line)\n",
    "    for k in range(length):\n",
    "        w = line[k]\n",
    "        if not w in charset:            \n",
    "            charset[w]=id\n",
    "            id+=1\n",
    "            \n",
    "print(len(charset))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortedcharset = sorted(set(alltext))\n",
    "char_indices = dict((c, i) for i, c in enumerate(sortedcharset))\n",
    "indices_char = dict((i, c) for i, c in enumerate(sortedcharset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在把原文按照指定长度划分为虚拟的句子。这个指定虚拟句子的长度一般使用平均句子的字数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.36245216367366\n",
      "13588\n"
     ]
    }
   ],
   "source": [
    "sentencelength = 0\n",
    "k=0\n",
    "for line in alltext0:\n",
    "    k=k+1\n",
    "    linelength = len(line)\n",
    "    sentencelength = (k-1)/k * sentencelength + linelength / k\n",
    "print(sentencelength)  \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 237154\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(alltext) - maxlen, step):\n",
    "    sentences.append(alltext[i: i + maxlen])\n",
    "    next_chars.append(alltext[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面对虚拟句子进行矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Finished initialization...\n",
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n",
      "120000\n",
      "150000\n",
      "180000\n",
      "210000\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(sortedcharset)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(sortedcharset)), dtype=np.bool)\n",
    "print('Finished initialization...')\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if (i % 30000 == 0):\n",
    "        print(i)  \n",
    "    for t in range(maxlen):\n",
    "        char=sentence[t]\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237154, 40, 3545)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是这么直接构造得是非常浪费空间的密集矩阵，这个矩阵占据大约30GB的内存，如果把句长再增加一些，那么在很多机器上无法运行。同时这么大的数据无法送给显卡进行计算，需要每次取一小块批量供GPU计算所需。这时候需要使用fit_generator方法，而不是原来的fit方法。fit_generator将每个batch的数据读入，从原始数据的稀疏矩阵变为当前批量的密集矩阵，然后计算。这样对内存的压力大大降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data generator for fit_generator method\n",
    "def data_generator(X, y, batch_size):\n",
    "    if batch_size<1:\n",
    "       batch_size=256    \n",
    "    number_of_batches = X.shape[0]//batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)    \n",
    "    #reset generator\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = (X[index_batch,:,:]).astype('float32')        \n",
    "        y_batch = (y[index_batch,:]).astype('float32')       \n",
    "        counter += 1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n",
      "段\n"
     ]
    }
   ],
   "source": [
    "#char=subsentences[j][1]\n",
    "char = sentences[1][1]\n",
    "print(char_indices[char])\n",
    "print(next_chars[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Finished compiling\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (300, 256)                3893248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (300, 3545)               911065    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (300, 3545)               0         \n",
      "=================================================================\n",
      "Total params: 4,804,313\n",
      "Trainable params: 4,804,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "batch_size=300\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, batch_size=batch_size,  input_shape=(maxlen, len(sortedcharset)), recurrent_dropout=0.1, dropout=0.1))\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(len(sortedcharset)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "adamoptimizer = keras.optimizers.Adam(lr = 1e-4)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adamoptimizer)\n",
    "print('Finished compiling')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "790/790 [==============================] - 373s - loss: 6.1863   \n",
      "Epoch 2/25\n",
      "790/790 [==============================] - 374s - loss: 5.9802   \n",
      "Epoch 3/25\n",
      "790/790 [==============================] - 353s - loss: 5.9708   \n",
      "Epoch 4/25\n",
      "790/790 [==============================] - 352s - loss: 5.9615   \n",
      "Epoch 5/25\n",
      "790/790 [==============================] - 345s - loss: 5.9506   \n",
      "Epoch 6/25\n",
      "790/790 [==============================] - 345s - loss: 5.9265   \n",
      "Epoch 7/25\n",
      "790/790 [==============================] - 346s - loss: 5.9017   \n",
      "Epoch 8/25\n",
      "790/790 [==============================] - 346s - loss: 5.8630   \n",
      "Epoch 9/25\n",
      "790/790 [==============================] - 347s - loss: 5.8232   \n",
      "Epoch 10/25\n",
      "790/790 [==============================] - 346s - loss: 5.7809   \n",
      "Epoch 11/25\n",
      "790/790 [==============================] - 345s - loss: 5.7303   \n",
      "Epoch 12/25\n",
      "790/790 [==============================] - 347s - loss: 5.6713   \n",
      "Epoch 13/25\n",
      "790/790 [==============================] - 345s - loss: 5.6130   \n",
      "Epoch 14/25\n",
      "790/790 [==============================] - 346s - loss: 5.5567   \n",
      "Epoch 15/25\n",
      "790/790 [==============================] - 346s - loss: 5.5145   \n",
      "Epoch 16/25\n",
      "790/790 [==============================] - 346s - loss: 5.4485   \n",
      "Epoch 17/25\n",
      "790/790 [==============================] - 347s - loss: 5.4053   \n",
      "Epoch 18/25\n",
      "790/790 [==============================] - 347s - loss: 5.3576   \n",
      "Epoch 19/25\n",
      "790/790 [==============================] - 346s - loss: 5.3055   \n",
      "Epoch 20/25\n",
      "790/790 [==============================] - 346s - loss: 5.2549   \n",
      "Epoch 21/25\n",
      "790/790 [==============================] - 346s - loss: 5.2279   \n",
      "Epoch 22/25\n",
      "790/790 [==============================] - 346s - loss: 5.1878   \n",
      "Epoch 23/25\n",
      "790/790 [==============================] - 345s - loss: 5.1484   \n",
      "Epoch 24/25\n",
      "790/790 [==============================] - 346s - loss: 5.1191   \n",
      "Epoch 25/25\n",
      "790/790 [==============================] - 346s - loss: 5.0717   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a5df93bb00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(X, y, batch_size=batch_size), \n",
    "                    steps_per_epoch=X.shape[0]//batch_size, \n",
    "                    epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一句，小顺儿的妈点一次头，或说一声“是”。老人的话，她已经听过起码有五十次，但是\n",
      "=================\n",
      "还 与 这 中 声 明 准 变 只 ， 他 小 适 风 先 手 ， 用 以 她\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "start_index=2799\n",
    "sentence = alltext[start_index: start_index + maxlen]\n",
    "sentence0=sentence\n",
    "x = np.zeros((1, maxlen, len(sortedcharset)))\n",
    "\n",
    "\n",
    "generated=''\n",
    "x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')\n",
    "for t, char in enumerate(sentence):\n",
    "     x[0, t, char_indices[char]] = 1.\n",
    "for i in range(20):\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.9)\n",
    "    next_char = indices_char[next_index]\n",
    "    generated+=next_char\n",
    "    sentence = sentence[1:]+next_char  \n",
    "\n",
    "print(sentence0)\n",
    "print(\"=================\")\n",
    "print(' '.join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(X, y)\n",
    "#del(model)\n",
    "\n",
    "for i in range(19):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n",
      "((10240, 40, 3545), (10240, 3545))\n"
     ]
    }
   ],
   "source": [
    "batch_size=10240\n",
    "number_of_batches = len(sentences)//batch_size\n",
    "counter=0\n",
    "shuffle_index = np.arange(len(sentences))\n",
    "np.random.shuffle(shuffle_index)    \n",
    "\n",
    "#reset generator\n",
    "\n",
    "for i in range(number_of_batches):\n",
    "    index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "    subsentences = [sentences[s] for s in index_batch]\n",
    "    X = np.zeros((batch_size, maxlen, len(sortedcharset)), dtype=np.bool)\n",
    "    y = np.zeros((batch_size, len(sortedcharset)), dtype=np.bool)\n",
    "    for j in range(len(subsentences)):\n",
    "        for t in range(maxlen):\n",
    "            char=subsentences[j][t]\n",
    "            X[j, t, char_indices[char]] = 1\n",
    "        y[j, char_indices[next_chars[j]]] = 1\n",
    "    X = X.astype('float32')        \n",
    "    y = y.astype('float32')       \n",
    "    counter += 1\n",
    "    print( (X.shape, y.shape ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是这种方法仍然需要一开始生成巨大的特征矩阵和因变量矩阵。我们可以将生成这两个矩阵的操作移入数据生成器中，这样无需产生大量数据等待输入GPU，而是每次只取所需并生成相应的矩阵并即刻输入GPU运算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Finished compiling\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (300, 256)                3893248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (300, 3545)               911065    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (300, 3545)               0         \n",
      "=================================================================\n",
      "Total params: 4,804,313\n",
      "Trainable params: 4,804,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "batch_size=300\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, batch_size=batch_size,  input_shape=(maxlen, len(sortedcharset)), recurrent_dropout=0.1, dropout=0.1))\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(len(sortedcharset)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "adamoptimizer = keras.optimizers.Adam(lr = 1e-4)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adamoptimizer)\n",
    "print('Finished compiling')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator2(sentences, sortedcharset, char_indices, maxlen=40, batch_size=256):\n",
    "    if batch_size<1:\n",
    "       batch_size=256    \n",
    "    number_of_batches = len(sentences)//batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(len(sentences))\n",
    "    np.random.shuffle(shuffle_index)    \n",
    "    #reset generator\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        subsentences = [sentences[s] for s in index_batch]\n",
    "        X = np.zeros((batch_size, maxlen, len(sortedcharset)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(sortedcharset)), dtype=np.bool)\n",
    "        for j, sentence in enumerate(subsentences):\n",
    "            for t in range(maxlen):\n",
    "                char=sentence[t]\n",
    "                X[j, t, char_indices[char]] = 1\n",
    "            y[j, char_indices[next_chars[j]]] = 1        \n",
    "        X = X.astype('float32')        \n",
    "        y = y.astype('float32')       \n",
    "        counter += 1\n",
    "        yield((np.array(X), np.array(y)))\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "790/790 [==============================] - 346s - loss: 5.0422   \n",
      "Epoch 2/25\n",
      "790/790 [==============================] - 353s - loss: 4.7564   \n",
      "Epoch 3/25\n",
      "790/790 [==============================] - 359s - loss: 4.7554   \n",
      "Epoch 4/25\n",
      "790/790 [==============================] - 361s - loss: 4.7551   \n",
      "Epoch 5/25\n",
      "790/790 [==============================] - 348s - loss: 4.7550   \n",
      "Epoch 6/25\n",
      "790/790 [==============================] - 341s - loss: 4.7549   \n",
      "Epoch 7/25\n",
      "790/790 [==============================] - 345s - loss: 4.7548   \n",
      "Epoch 8/25\n",
      "790/790 [==============================] - 347s - loss: 4.7548   \n",
      "Epoch 9/25\n",
      "790/790 [==============================] - 352s - loss: 4.7548   \n",
      "Epoch 10/25\n",
      "790/790 [==============================] - 350s - loss: 4.7548   \n",
      "Epoch 11/25\n",
      "790/790 [==============================] - 335s - loss: 4.7548   \n",
      "Epoch 12/25\n",
      "790/790 [==============================] - 349s - loss: 4.7548   \n",
      "Epoch 13/25\n",
      "790/790 [==============================] - 348s - loss: 4.7548   \n",
      "Epoch 14/25\n",
      "790/790 [==============================] - 353s - loss: 4.7548   \n",
      "Epoch 15/25\n",
      "790/790 [==============================] - 349s - loss: 4.7548   \n",
      "Epoch 16/25\n",
      "790/790 [==============================] - 366s - loss: 4.7548   \n",
      "Epoch 17/25\n",
      "790/790 [==============================] - 374s - loss: 4.7548   \n",
      "Epoch 18/25\n",
      "790/790 [==============================] - 347s - loss: 4.7548   \n",
      "Epoch 19/25\n",
      "790/790 [==============================] - 362s - loss: 4.7548   \n",
      "Epoch 20/25\n",
      "790/790 [==============================] - 353s - loss: 4.7548   \n",
      "Epoch 21/25\n",
      "790/790 [==============================] - 357s - loss: 4.7548   \n",
      "Epoch 22/25\n",
      "790/790 [==============================] - 358s - loss: 4.7548   \n",
      "Epoch 23/25\n",
      "790/790 [==============================] - 370s - loss: 4.7548   \n",
      "Epoch 24/25\n",
      "790/790 [==============================] - 362s - loss: 4.7548   \n",
      "Epoch 25/25\n",
      "790/790 [==============================] - 362s - loss: 4.7548   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d180d2c0b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator2(sentences, sortedcharset, char_indices, maxlen=maxlen, batch_size=batch_size), \n",
    "                    steps_per_epoch=len(sentences)//batch_size, \n",
    "                    epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一句，小顺儿的妈点一次头，或说一声“是”。老人的话，她已经听过起码有五十次，但是\n",
      "=================\n",
      "为 蓝 三 以 各 有 与 么 样 避 ， 3 就 目 字 里 三 目 在 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "start_index=2799\n",
    "sentence = alltext[start_index: start_index + maxlen]\n",
    "sentence0=sentence\n",
    "x = np.zeros((1, maxlen, len(sortedcharset)))\n",
    "\n",
    "\n",
    "generated=''\n",
    "x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')\n",
    "for t, char in enumerate(sentence):\n",
    "     x[0, t, char_indices[char]] = 1.\n",
    "for i in range(20):\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 1.1)\n",
    "    next_char = indices_char[next_index]\n",
    "    generated+=next_char\n",
    "    sentence = sentence[1:]+next_char  \n",
    "\n",
    "print(sentence0)\n",
    "print(\"=================\")\n",
    "print(' '.join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_index=2799\n",
    "sentence = alltext[start_index: start_index + maxlen]\n",
    "sentence0=sentence\n",
    "x = np.zeros((1, maxlen, len(sortedcharset)))\n",
    "\n",
    "def GenSentence(original):\n",
    "    sentence=original\n",
    "    generated=''\n",
    "    for i in range(20):\n",
    "        x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, 1.20)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated+=next_char\n",
    "        sentence = sentence[1:]+next_char  \n",
    "    return(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有四五堂功课呢。”她回答。\n",
      "\n",
      "“哼！开了炮，还不快快的回来！瑞丰和他的那个疯娘们----->会地看纳随《—会十的消又：一三系平神各就\n",
      "==========\n",
      "会地看纳随《—会十的消又：一三系平神各就------>二避一么在，不以直许标家独看，而什枪国只\n"
     ]
    }
   ],
   "source": [
    "start_index=3041\n",
    "sentence0 = alltext[start_index: start_index + maxlen]\n",
    "generated0 = GenSentence(sentence0)\n",
    "print(sentence0+\"----->\"+generated0)\n",
    "print(\"==========\")\n",
    "generated1 = GenSentence(generated0)\n",
    "print(generated0+\"------>\"+generated1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects not found...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(X, y, model)\n",
    "except:\n",
    "    print('Objects not found...')\n",
    "    \n",
    "for i in range(10):\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
