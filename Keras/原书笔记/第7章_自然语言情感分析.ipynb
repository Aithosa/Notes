{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np \n",
    "from keras.datasets import imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "           65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "            5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "            9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "          167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "           17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "            6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "          469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "           38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "           17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "           12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "           16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "           38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "           25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "           52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "          107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "            5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "          317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "            4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "          141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "          134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "           51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "           65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "           16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "          178,    32]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(X_train[0], (1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.71364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len = list(map(len, X_train))\n",
    "np.mean(avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13acbb11668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(avg_len, bins=range(min(avg_len), max(avg_len) + 50, 50)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Flatten \n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.preprocessing import sequence \n",
    "import keras \n",
    "import numpy as np \n",
    "from keras.datasets import imdb \n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data() \n",
    "m=max(list(map(len, X_train))+ list(map(len, X_test)))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxword = 400\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = maxword)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = maxword)\n",
    "vocab_size = np.max([np.max(X_train[i]) for i in range(X_train.shape[0])]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length = maxword))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 400, 64)           5669568   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               12800500  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 18,830,869\n",
      "Trainable params: 18,830,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.4514 - acc: 0.7513 - val_loss: 0.2960 - val_acc: 0.8750\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0963 - acc: 0.9676 - val_loss: 0.3743 - val_acc: 0.8601\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.6467 - val_acc: 0.8591\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 10s - loss: 5.1717e-04 - acc: 0.9999 - val_loss: 1.0052 - val_acc: 0.8559\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.6195e-06 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.8570\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 10s - loss: 3.2650e-06 - acc: 1.0000 - val_loss: 1.0866 - val_acc: 0.8564\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 10s - loss: 1.9993e-06 - acc: 1.0000 - val_loss: 1.1213 - val_acc: 0.8561\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 10s - loss: 1.1644e-06 - acc: 1.0000 - val_loss: 1.1707 - val_acc: 0.8570\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 10s - loss: 6.6276e-07 - acc: 1.0000 - val_loss: 1.2164 - val_acc: 0.8574\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 10s - loss: 3.9984e-07 - acc: 1.0000 - val_loss: 1.2884 - val_acc: 0.8564\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 10s - loss: 1.5278e-07 - acc: 1.0000 - val_loss: 1.3796 - val_acc: 0.8566\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 10s - loss: 1.0723e-07 - acc: 1.0000 - val_loss: 1.4194 - val_acc: 0.8568\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 10s - loss: 9.2242e-08 - acc: 1.0000 - val_loss: 1.4465 - val_acc: 0.8568\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 10s - loss: 8.4448e-08 - acc: 1.0000 - val_loss: 1.4663 - val_acc: 0.8566\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.9802e-08 - acc: 1.0000 - val_loss: 1.4831 - val_acc: 0.8566\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.6812e-08 - acc: 1.0000 - val_loss: 1.4976 - val_acc: 0.8566\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.4742e-08 - acc: 1.0000 - val_loss: 1.5098 - val_acc: 0.8566\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.3247e-08 - acc: 1.0000 - val_loss: 1.5203 - val_acc: 0.8566\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.2189e-08 - acc: 1.0000 - val_loss: 1.5304 - val_acc: 0.8565\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 10s - loss: 7.1346e-08 - acc: 1.0000 - val_loss: 1.5390 - val_acc: 0.8564\n",
      "24800/25000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 20,batch_size = 100, verbose = 1)\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5390439252797141, 0.85643999999999998]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 64)           5669568   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 200, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                819264    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 6,528,001\n",
      "Trainable params: 6,528,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.preprocessing import sequence \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length = maxword))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters = 128, kernel_size = 3, padding = 'same',activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 12s - loss: 0.4860 - acc: 0.7189 - val_loss: 0.2693 - val_acc: 0.8892\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.2236 - acc: 0.9138 - val_loss: 0.2645 - val_acc: 0.8916\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.1587 - acc: 0.9424 - val_loss: 0.3100 - val_acc: 0.8764\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.1142 - acc: 0.9597 - val_loss: 0.3152 - val_acc: 0.8799\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0890 - acc: 0.9699 - val_loss: 0.3662 - val_acc: 0.8758\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0641 - acc: 0.9790 - val_loss: 0.4649 - val_acc: 0.8646\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0493 - acc: 0.9840 - val_loss: 0.4959 - val_acc: 0.8565\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0383 - acc: 0.9884 - val_loss: 0.5187 - val_acc: 0.8586\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0267 - acc: 0.9916 - val_loss: 0.7127 - val_acc: 0.8458\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0198 - acc: 0.9939 - val_loss: 1.1308 - val_acc: 0.830004 - ET\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0123 - acc: 0.9963 - val_loss: 0.8761 - val_acc: 0.8527\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0090 - acc: 0.9977 - val_loss: 1.0457 - val_acc: 0.8488\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0072 - acc: 0.9982 - val_loss: 1.1228 - val_acc: 0.8425\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0051 - acc: 0.9993 - val_loss: 1.8203 - val_acc: 0.8360\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0036 - acc: 0.9992 - val_loss: 1.7642 - val_acc: 0.8458acc: \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0043 - acc: 0.9995 - val_loss: 1.7908 - val_acc: 0.8468\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0021 - acc: 0.9998 - val_loss: 1.9277 - val_acc: 0.8447\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0122 - acc: 0.9987 - val_loss: 1.7697 - val_acc: 0.8481\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0035 - acc: 0.9996 - val_loss: 1.8187 - val_acc: 0.8444\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 10s - loss: 0.0056 - acc: 0.9991 - val_loss: 1.7834 - val_acc: 0.8423\n",
      "24960/25000 [============================>.] - ETA: 0s[1.7833950957139582, 0.84228000000000003]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 20, batch_size = 100)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7833950957139582, 0.84228000000000003]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 400, 64)           5669568   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 400, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 400, 64)           49408     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,830,241\n",
      "Trainable params: 5,830,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length = maxword))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 546s - loss: 0.5645 - acc: 0.6731 - val_loss: 0.3635 - val_acc: 0.8475\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 538s - loss: 0.3497 - acc: 0.8590 - val_loss: 0.3994 - val_acc: 0.8179\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 537s - loss: 0.2460 - acc: 0.9079 - val_loss: 0.3249 - val_acc: 0.8659\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 534s - loss: 0.1802 - acc: 0.9357 - val_loss: 0.3177 - val_acc: 0.8644\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 534s - loss: 0.1425 - acc: 0.9505 - val_loss: 0.3417 - val_acc: 0.8508\n",
      "25000/25000 [==============================] - 461s   \n",
      "[0.34172198061943054, 0.85075999999999996]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 5, batch_size = 100)\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
